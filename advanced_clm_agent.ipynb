{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9055cd0-a561-4e66-b551-47ede98c4e74",
   "metadata": {},
   "source": [
    "# LangGraph Based CLM and Google Data Commons Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f310d4-a340-44ca-8470-31e73892dfb2",
   "metadata": {},
   "source": [
    "## 0. Start Data Commons MCP Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b3dea-3bbc-4908-aba2-8428980462f5",
   "metadata": {},
   "source": [
    "Open a terminal and run the following commands to start the Data Commons MCP Server \n",
    "```\n",
    "wget https://astral.sh/uv/install.sh\n",
    "mv install.sh uv_install.sh\n",
    "sh uv_install.sh \n",
    "export DC_API_KEY=your data commons API key \n",
    "uv tool run datacommons-mcp serve http --port 3000 &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe84ea-1a78-44eb-b2f0-1af4c88a1a2b",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ca3a7-f3be-4552-a6ae-e17dfdfd7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph langchain-openai langchain-core langsmith \\\n",
    "    python-dotenv aiohttp folium matplotlib markdown nest-asyncio \\\n",
    "    ipywidgets pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8aa594-ce84-4897-b279-e4ef886d44f5",
   "metadata": {},
   "source": [
    "## 2. Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832be22-2370-4d8c-bc2d-81c5d17c6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import TypedDict, Annotated, Sequence, Literal\n",
    "import operator\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================================\n",
    "# LangSmith Configuration - THIS IS THE MAGIC! \n",
    "# ============================================================================\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"Your LangChain API Key\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"pr-shadowy-oleo-13\"\n",
    "\n",
    "print(\"LangSmith tracing enabled!\")\n",
    "print(f\"   Project: {os.environ['LANGCHAIN_PROJECT']}\")\n",
    "print(f\"   View traces at: https://smith.langchain.com/\")\n",
    "\n",
    "# API Keys\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "DC_API_KEY = os.getenv('DC_API_KEY')\n",
    "NRP_API_KEY = os.getenv('NRP_API_KEY')\n",
    "\n",
    "# Base configuration for CLM GeoServer\n",
    "CLM_CONFIG = {\n",
    "    \"wcs_base_url\": \"https://sparcal.sdsc.edu/geoserver\",\n",
    "    \"wfs_base_url\": \"https://sparcal.sdsc.edu/geoserver/boundary/wfs\",\n",
    "    \"feature_id\": \"boundary:ca_counties\",\n",
    "    \"filter_column\": \"name\"\n",
    "}\n",
    "\n",
    "# MCP Configuration\n",
    "MCP_URL = \"https://wenokn.fastmcp.app/mcp\"\n",
    "DC_MCP_URL = \"http://localhost:3000/mcp\"\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   OpenAI: {'‚úì' if OPENAI_API_KEY else '‚úó'}\")\n",
    "print(f\"   Data Commons: {'‚úì' if DC_API_KEY else '‚úó'}\")\n",
    "print(f\"   NRP: {'‚úì' if NRP_API_KEY else '‚úó'}\")\n",
    "\n",
    "# Test LangSmith connection\n",
    "from langsmith import Client\n",
    "\n",
    "try:\n",
    "    client = Client()\n",
    "    print(\"‚úÖ LangSmith connection successful!\")\n",
    "    print(f\"   API Key is valid\")\n",
    "    print(f\"   Project: {os.environ.get('LANGCHAIN_PROJECT')}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LangSmith connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e084e7-f575-4dfb-bd26-3e8b0be5d308",
   "metadata": {},
   "source": [
    "## 3. State Definition (LangGraph Core Concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c1160-9bb0-4e44-ad28-05762b7d917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from typing import Literal\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    The state of our multi-agent system.\n",
    "    This gets passed between all nodes in the graph.\n",
    "    \"\"\"\n",
    "    # Core conversation\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "    # Routing decision\n",
    "    next_agent: Literal[\"clm\", \"dc\", \"both\", \"END\"]\n",
    "    \n",
    "    # Agent responses\n",
    "    clm_response: str\n",
    "    dc_response: str\n",
    "    \n",
    "    # Visualizations\n",
    "    map_data: dict | None\n",
    "    distribution_data: dict | None\n",
    "    \n",
    "    # Metadata\n",
    "    question: str\n",
    "    routing_confidence: float\n",
    "    routing_reasoning: str\n",
    "\n",
    "print(\"‚úÖ State schema defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7cc78-cd70-4ddb-b364-ed326df2caf4",
   "metadata": {},
   "source": [
    "## 4. MCP Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb1bc6e-686c-408a-b4cc-f3f05f90da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "class MCPClient:\n",
    "    \"\"\"MCP Client with automatic LangSmith tracing.\"\"\"\n",
    "    \n",
    "    def __init__(self, url: str, name: str = \"MCP\"):\n",
    "        self.url = url\n",
    "        self.name = name\n",
    "        self.session: aiohttp.ClientSession | None = None\n",
    "    \n",
    "    async def __aenter__(self):\n",
    "        self.session = aiohttp.ClientSession()\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, *exc):\n",
    "        if self.session:\n",
    "            await self.session.close()\n",
    "    \n",
    "    async def _parse_sse_response(self, resp: aiohttp.ClientResponse) -> List[Dict]:\n",
    "        \"\"\"Parse server-sent events response.\"\"\"\n",
    "        messages = []\n",
    "        buffer = \"\"\n",
    "        \n",
    "        async for line in resp.content:\n",
    "            line = line.decode(\"utf-8\").strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"data: \"):\n",
    "                buffer += line[6:] + \"\\n\"\n",
    "            elif line == \"data: [DONE]\":\n",
    "                if buffer.strip():\n",
    "                    try:\n",
    "                        messages.append(json.loads(buffer.strip()))\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "                buffer = \"\"\n",
    "                break\n",
    "        \n",
    "        if buffer.strip():\n",
    "            try:\n",
    "                messages.append(json.loads(buffer.strip()))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    async def call_tool(self, tool_name: str, arguments: Dict) -> Any:\n",
    "        \"\"\"\n",
    "        Call MCP tool.\n",
    "        LangSmith will automatically trace this as a tool call!\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"jsonrpc\": \"2.0\",\n",
    "            \"id\": 3,\n",
    "            \"method\": \"tools/call\",\n",
    "            \"params\": {\"name\": tool_name, \"arguments\": arguments}\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json, text/event-stream\"\n",
    "        }\n",
    "        \n",
    "        async with self.session.post(self.url, json=payload, headers=headers) as resp:\n",
    "            if resp.status != 200:\n",
    "                raise RuntimeError(f\"MCP HTTP {resp.status}\")\n",
    "            \n",
    "            msgs = await self._parse_sse_response(resp)\n",
    "            if not msgs:\n",
    "                raise RuntimeError(\"Empty MCP response\")\n",
    "            \n",
    "            result = msgs[0].get(\"result\", {})\n",
    "            \n",
    "            # Extract text content\n",
    "            text_parts = [\n",
    "                block.get(\"text\", \"\")\n",
    "                for block in result.get(\"content\", [])\n",
    "                if block.get(\"type\") == \"text\"\n",
    "            ]\n",
    "            \n",
    "            return \"\\n\".join(text_parts) or json.dumps(result, indent=2)\n",
    "\n",
    "# Initialize MCP clients\n",
    "clm_mcp = MCPClient(MCP_URL, \"CLM-MCP\")\n",
    "dc_mcp = MCPClient(DC_MCP_URL, \"DC-MCP\")\n",
    "\n",
    "print(\"‚úÖ MCP clients initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376750f3-a858-48e0-95e4-be735c857ac0",
   "metadata": {},
   "source": [
    "## 5. CLM Agent Tools (as LangChain Tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae24f8-9a32-4079-9555-02ba9fe2c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langsmith import traceable\n",
    "\n",
    "@tool\n",
    "@traceable(name=\"search_clm_datasets\")\n",
    "async def search_clm_datasets(query: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Search for California Landscape Metrics datasets.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query for datasets\n",
    "        top_k: Number of top results to return\n",
    "    \"\"\"\n",
    "    async with clm_mcp:\n",
    "        result = await clm_mcp.call_tool(\n",
    "            \"search_datasets\",\n",
    "            {\"query\": query, \"top_k\": top_k}\n",
    "        )\n",
    "        \n",
    "        # Parse result\n",
    "        try:\n",
    "            data = json.loads(result) if isinstance(result, str) else result\n",
    "            if data.get('success') and data.get('datasets'):\n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'selected': data['datasets'][0],\n",
    "                    'alternatives': data['datasets'][1:]\n",
    "                }\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return {'success': False, 'error': 'No datasets found'}\n",
    "\n",
    "@tool\n",
    "@traceable(name=\"get_clm_statistics\")\n",
    "async def get_clm_statistics(\n",
    "    coverage_id: str,\n",
    "    counties: List[str] | None = None,\n",
    "    stats: List[str] | None = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get statistical measures for CLM dataset.\n",
    "    \n",
    "    Args:\n",
    "        coverage_id: WCS coverage ID for the dataset\n",
    "        counties: List of county names (None for all)\n",
    "        stats: Statistics to compute (default: mean, median, min, max, std)\n",
    "    \"\"\"\n",
    "    if stats is None:\n",
    "        stats = [\"mean\", \"median\", \"min\", \"max\", \"std\"]\n",
    "    \n",
    "    async with clm_mcp:\n",
    "        result = await clm_mcp.call_tool(\n",
    "            \"compute_zonal_stats\",\n",
    "            {\n",
    "                **CLM_CONFIG,\n",
    "                \"wcs_coverage_id\": coverage_id,\n",
    "                \"filter_value\": counties,\n",
    "                \"stats\": stats\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            return json.loads(result) if isinstance(result, str) else result\n",
    "        except:\n",
    "            return {'success': False, 'error': str(result)[:200]}\n",
    "\n",
    "@tool\n",
    "@traceable(name=\"get_clm_distribution\")\n",
    "async def get_clm_distribution(\n",
    "    coverage_id: str,\n",
    "    counties: List[str] | None = None,\n",
    "    num_bins: int = 10\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get value distribution for CLM dataset.\n",
    "    \n",
    "    Args:\n",
    "        coverage_id: WCS coverage ID\n",
    "        counties: County names (None for all)\n",
    "        num_bins: Number of histogram bins\n",
    "    \"\"\"\n",
    "    async with clm_mcp:\n",
    "        result = await clm_mcp.call_tool(\n",
    "            \"zonal_distribution\",\n",
    "            {\n",
    "                **CLM_CONFIG,\n",
    "                \"wcs_coverage_id\": coverage_id,\n",
    "                \"filter_value\": counties,\n",
    "                \"num_bins\": num_bins,\n",
    "                \"global_bins\": True,\n",
    "                \"categorical_threshold\": 20\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(result) if isinstance(result, str) else result\n",
    "            if data.get('success'):\n",
    "                data['action'] = 'show_distribution'\n",
    "            return data\n",
    "        except:\n",
    "            return {'success': False, 'error': str(result)[:200]}\n",
    "\n",
    "# List of CLM tools\n",
    "clm_tools = [search_clm_datasets, get_clm_statistics, get_clm_distribution]\n",
    "\n",
    "print(\"‚úÖ CLM tools defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce467eb-10bd-4e51-8b4e-b18a04a04642",
   "metadata": {},
   "source": [
    "## 6. Data Commons Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1af995-5cce-4c45-ba26-f6f1dedb6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "@traceable(name=\"search_dc_indicators\")\n",
    "async def search_dc_indicators(\n",
    "    query: str,\n",
    "    places: List[str] | None = None,\n",
    "    parent_place: str | None = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Search for indicators in Google Data Commons.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        places: List of place names\n",
    "        parent_place: Parent geographic area\n",
    "    \"\"\"\n",
    "    async with dc_mcp:\n",
    "        args = {\n",
    "            \"query\": query,\n",
    "            \"include_topics\": True,\n",
    "            \"maybe_bilateral\": False\n",
    "        }\n",
    "        if places:\n",
    "            args[\"places\"] = places\n",
    "        if parent_place:\n",
    "            args[\"parent_place\"] = parent_place\n",
    "        \n",
    "        return await dc_mcp.call_tool(\"search_indicators\", args)\n",
    "\n",
    "@tool\n",
    "@traceable(name=\"get_dc_observations\")\n",
    "async def get_dc_observations(\n",
    "    variable_dcid: str,\n",
    "    place_dcid: str,\n",
    "    child_place_type: str | None = None,\n",
    "    date: str = \"latest\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get observations from Google Data Commons.\n",
    "    \n",
    "    Args:\n",
    "        variable_dcid: Variable DCID\n",
    "        place_dcid: Place DCID\n",
    "        child_place_type: Type of child places\n",
    "        date: Date (default: \"latest\")\n",
    "    \"\"\"\n",
    "    async with dc_mcp:\n",
    "        args = {\n",
    "            \"variable_dcid\": variable_dcid,\n",
    "            \"place_dcid\": place_dcid,\n",
    "            \"date\": date\n",
    "        }\n",
    "        if child_place_type:\n",
    "            args[\"child_place_type\"] = child_place_type\n",
    "        \n",
    "        return await dc_mcp.call_tool(\"get_observations\", args)\n",
    "\n",
    "# List of DC tools\n",
    "dc_tools = [search_dc_indicators, get_dc_observations]\n",
    "\n",
    "print(\"‚úÖ Data Commons tools defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61984b8-331b-44e9-abed-eccbbc2a3dca",
   "metadata": {},
   "source": [
    "## 7. LangGraph Nodes - The Agent Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe3983-597a-4e2e-b988-d03e02eb160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langsmith import traceable\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ============================================================================\n",
    "# Node 1: Router - Decides which agent(s) to use\n",
    "# ============================================================================\n",
    "\n",
    "@traceable(name=\"Router Node\")\n",
    "async def router_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Route the question to appropriate agent(s).\n",
    "    This is automatically traced in LangSmith!\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Define system message without template variables in JSON\n",
    "    system_message = \"\"\"You are a routing expert for a multi-agent system.\n",
    "\n",
    "**Agent Capabilities:**\n",
    "\n",
    "**CLM Agent** - California landscape/environmental data:\n",
    "- 189 datasets: air quality, biodiversity, carbon, fire, water, poverty, unemployment\n",
    "- 30m x 30m resolution spatial data\n",
    "- Maps, distributions, county statistics\n",
    "- California ONLY\n",
    "\n",
    "**DC Agent** - Global demographic/economic data:\n",
    "- Any location worldwide\n",
    "- Population, income, health, economics\n",
    "- Aggregated totals and rates\n",
    "\n",
    "**Routing Rules:**\n",
    "1. \"distribution\" / \"map\" / \"spatial pattern\" ‚Üí CLM (if California topic)\n",
    "2. \"total count\" / \"how many people\" ‚Üí DC\n",
    "3. \"rate\" / \"percentage\" without \"distribution\" ‚Üí DC (actual demographic rate)\n",
    "4. California environmental topics ‚Üí CLM\n",
    "5. Non-California locations ‚Üí DC\n",
    "6. If unsure ‚Üí BOTH\n",
    "\n",
    "Respond with JSON only (no markdown):\n",
    "{{\n",
    "    \"agent\": \"clm\" or \"dc\" or \"both\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"reasoning\": \"brief explanation\"\n",
    "}}\"\"\"\n",
    "    \n",
    "    router_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_message),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    \n",
    "    response = await llm.ainvoke(\n",
    "        router_prompt.format_messages(question=question)\n",
    "    )\n",
    "    \n",
    "    # Parse routing decision\n",
    "    try:\n",
    "        import json\n",
    "        decision = json.loads(response.content)\n",
    "        next_agent = decision.get(\"agent\", \"both\")\n",
    "        confidence = decision.get(\"confidence\", 0.5)\n",
    "        reasoning = decision.get(\"reasoning\", \"\")\n",
    "    except:\n",
    "        next_agent = \"both\"\n",
    "        confidence = 0.5\n",
    "        reasoning = \"Failed to parse routing decision\"\n",
    "    \n",
    "    print(f\"üéØ Router Decision: {next_agent.upper()} (confidence: {confidence:.0%})\")\n",
    "    print(f\"   Reasoning: {reasoning}\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"next_agent\": next_agent,\n",
    "        \"routing_confidence\": confidence,\n",
    "        \"routing_reasoning\": reasoning,\n",
    "        \"messages\": [response]\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# Node 2: CLM Agent - California Landscape Metrics\n",
    "# ============================================================================\n",
    "\n",
    "@traceable(name=\"CLM Agent Node\")\n",
    "async def clm_agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    CLM Agent with tool calling.\n",
    "    All tool calls are automatically traced in LangSmith!\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    clm_prompt = f\"\"\"You are an expert in California Landscape Metrics datasets.\n",
    "\n",
    "You have access to tools for:\n",
    "1. Searching 189 CLM datasets\n",
    "2. Computing statistics for counties\n",
    "3. Getting value distributions\n",
    "\n",
    "**Workflow:**\n",
    "1. First call search_clm_datasets to find relevant dataset\n",
    "2. Then use get_clm_statistics or get_clm_distribution as needed\n",
    "\n",
    "**Important:**\n",
    "- Always mention dataset name in response\n",
    "- Clarify that CLM data is 30m x 30m spatial resolution\n",
    "- For distributions, specify units and meaning\n",
    "\n",
    "Answer this question: {question}\"\"\"\n",
    "    \n",
    "    # Create ReAct agent with tools\n",
    "    # This gives us automatic tool calling with LangSmith tracing!\n",
    "    clm_react_agent = create_react_agent(\n",
    "        llm,\n",
    "        clm_tools,\n",
    "        prompt=clm_prompt\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = await clm_react_agent.ainvoke({\n",
    "            \"messages\": [HumanMessage(content=question)]\n",
    "        })\n",
    "        \n",
    "        response = result[\"messages\"][-1].content\n",
    "        \n",
    "        # Check for visualizations in tool results\n",
    "        map_data = None\n",
    "        distribution_data = None\n",
    "        \n",
    "        for msg in result.get(\"messages\", []):\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                try:\n",
    "                    tool_result = json.loads(msg.content) if isinstance(msg.content, str) else msg.content\n",
    "                    if tool_result.get(\"action\") == \"show_distribution\":\n",
    "                        distribution_data = tool_result\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        print(f\"‚úÖ CLM Agent completed\")\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"clm_response\": response,\n",
    "            \"map_data\": map_data,\n",
    "            \"distribution_data\": distribution_data,\n",
    "            \"messages\": state[\"messages\"] + result[\"messages\"]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"CLM Agent error: {str(e)}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"clm_response\": error_msg,\n",
    "            \"messages\": state[\"messages\"] + [AIMessage(content=error_msg)]\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# Node 3: Data Commons Agent\n",
    "# ============================================================================\n",
    "\n",
    "@traceable(name=\"DC Agent Node\")\n",
    "async def dc_agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Data Commons Agent with tool calling.\n",
    "    Automatically traced in LangSmith!\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Enhance question with location qualifier\n",
    "    enhanced_question = question\n",
    "    if not any(x in question.lower() for x in [', ca', ', usa', 'california', ' county']):\n",
    "        # Try to extract location and add qualifier\n",
    "        import re\n",
    "        location_match = re.search(r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b', question)\n",
    "        if location_match:\n",
    "            location = location_match.group(1)\n",
    "            enhanced_question = question.replace(location, f\"{location}, CA, USA\")\n",
    "    \n",
    "    dc_prompt = f\"\"\"You are a precise data analyst using Google Data Commons.\n",
    "\n",
    "**Workflow:**\n",
    "1. Use search_dc_indicators to find relevant variable\n",
    "2. Use get_dc_observations to get the data\n",
    "\n",
    "**Rules:**\n",
    "- Always qualify place names: \"San Diego, CA, USA\"\n",
    "- Use date=\"latest\" unless specified\n",
    "- Provide actual demographic rates/totals\n",
    "\n",
    "Answer this question: {enhanced_question}\"\"\"\n",
    "    \n",
    "    dc_react_agent = create_react_agent(\n",
    "        llm,\n",
    "        dc_tools,\n",
    "        prompt=dc_prompt\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = await dc_react_agent.ainvoke({\n",
    "            \"messages\": [HumanMessage(content=enhanced_question)]\n",
    "        })\n",
    "        \n",
    "        response = result[\"messages\"][-1].content\n",
    "        \n",
    "        print(f\"‚úÖ DC Agent completed\")\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"dc_response\": response,\n",
    "            \"messages\": state[\"messages\"] + result[\"messages\"]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"DC Agent error: {str(e)}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"dc_response\": error_msg,\n",
    "            \"messages\": state[\"messages\"] + [AIMessage(content=error_msg)]\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# Node 4: Combiner - Merge responses from both agents\n",
    "# ============================================================================\n",
    "\n",
    "@traceable(name=\"Combiner Node\")\n",
    "async def combiner_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Combine responses from CLM and DC agents.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    clm_resp = state.get(\"clm_response\", \"\")\n",
    "    dc_resp = state.get(\"dc_response\", \"\")\n",
    "    \n",
    "    combiner_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You combine responses from specialized agents.\n",
    "\n",
    "**Agent Types:**\n",
    "- **CLM**: California spatial data (30m x 30m pixels) - shows geographic patterns\n",
    "- **DC**: Demographic totals/rates - shows population statistics\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify which responses are useful\n",
    "2. Combine complementary information\n",
    "3. Clarify differences (spatial patterns vs population rates)\n",
    "4. Always mention dataset names\n",
    "\n",
    "**Critical:**\n",
    "- CLM \"mean unemployment\" = spatial average across pixels, NOT actual unemployment rate\n",
    "- DC \"unemployment rate\" = actual demographic rate\n",
    "- Explain this distinction when both agents respond\"\"\"),\n",
    "        (\"human\", \"\"\"Question: {question}\n",
    "\n",
    "CLM Response: {clm_response}\n",
    "\n",
    "DC Response: {dc_response}\n",
    "\n",
    "Provide a clear, combined answer:\"\"\")\n",
    "    ])\n",
    "    \n",
    "    response = await llm.ainvoke(\n",
    "        combiner_prompt.format_messages(\n",
    "            question=question,\n",
    "            clm_response=clm_resp,\n",
    "            dc_response=dc_resp\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Combined responses\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": state[\"messages\"] + [response]\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ LangGraph nodes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3feeb6-3eef-4a64-9788-b2bfe38012b6",
   "metadata": {},
   "source": [
    "## 8. Build the LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b304a3-363d-4fc0-9236-eb4a010fcaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# ============================================================================\n",
    "# Graph Construction - This is where the magic happens! üé®\n",
    "# ============================================================================\n",
    "\n",
    "def build_agent_graph():\n",
    "    \"\"\"\n",
    "    Build the multi-agent workflow graph.\n",
    "    LangSmith will visualize this as a beautiful flowchart!\n",
    "    \"\"\"\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"router\", router_node)\n",
    "    workflow.add_node(\"clm_agent\", clm_agent_node)\n",
    "    workflow.add_node(\"dc_agent\", dc_agent_node)\n",
    "    workflow.add_node(\"combiner\", combiner_node)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"router\")\n",
    "    \n",
    "    # Conditional routing based on router decision\n",
    "    def route_after_router(state: AgentState) -> str:\n",
    "        next_agent = state.get(\"next_agent\", \"both\")\n",
    "        if next_agent == \"clm\":\n",
    "            return \"clm_agent\"\n",
    "        elif next_agent == \"dc\":\n",
    "            return \"dc_agent\"\n",
    "        else:  # \"both\"\n",
    "            return \"clm_agent\"  # Will run both in sequence\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"router\",\n",
    "        route_after_router,\n",
    "        {\n",
    "            \"clm_agent\": \"clm_agent\",\n",
    "            \"dc_agent\": \"dc_agent\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # After CLM agent\n",
    "    def route_after_clm(state: AgentState) -> str:\n",
    "        if state.get(\"next_agent\") == \"both\" and not state.get(\"dc_response\"):\n",
    "            return \"dc_agent\"\n",
    "        elif state.get(\"next_agent\") == \"both\":\n",
    "            return \"combiner\"\n",
    "        else:\n",
    "            return END\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"clm_agent\",\n",
    "        route_after_clm,\n",
    "        {\n",
    "            \"dc_agent\": \"dc_agent\",\n",
    "            \"combiner\": \"combiner\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # After DC agent\n",
    "    def route_after_dc(state: AgentState) -> str:\n",
    "        if state.get(\"next_agent\") == \"both\" and state.get(\"clm_response\"):\n",
    "            return \"combiner\"\n",
    "        else:\n",
    "            return END\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"dc_agent\",\n",
    "        route_after_dc,\n",
    "        {\n",
    "            \"combiner\": \"combiner\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # After combiner\n",
    "    workflow.add_edge(\"combiner\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Build the graph\n",
    "agent_graph = build_agent_graph()\n",
    "\n",
    "print(\"‚úÖ LangGraph compiled!\")\n",
    "print(\"   View graph structure at: https://smith.langchain.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf707ee-07b9-46c5-bdee-07321aede78b",
   "metadata": {},
   "source": [
    "## 9. Run Function with Full Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5119decb-d98b-476d-a445-4d11ca0367e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(name=\"Multi-Agent Query\")\n",
    "async def run_multi_agent_query(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run a question through the multi-agent system.\n",
    "    Everything is automatically traced in LangSmith!\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with output, visualizations, and metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìù Question: {question}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"messages\": [],\n",
    "        \"question\": question,\n",
    "        \"next_agent\": \"both\",\n",
    "        \"clm_response\": \"\",\n",
    "        \"dc_response\": \"\",\n",
    "        \"map_data\": None,\n",
    "        \"distribution_data\": None,\n",
    "        \"routing_confidence\": 0.0,\n",
    "        \"routing_reasoning\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Run the graph\n",
    "    # LangSmith will create a beautiful trace showing:\n",
    "    # - Router decision\n",
    "    # - Which agents ran\n",
    "    # - All tool calls\n",
    "    # - Token usage\n",
    "    # - Latency for each step\n",
    "    final_state = await agent_graph.ainvoke(initial_state)\n",
    "    \n",
    "    # Extract final response\n",
    "    final_message = final_state[\"messages\"][-1]\n",
    "    output = final_message.content if hasattr(final_message, 'content') else str(final_message)\n",
    "    \n",
    "    result = {\n",
    "        'output': output,\n",
    "        'map_data': final_state.get('map_data'),\n",
    "        'distribution_data': final_state.get('distribution_data'),\n",
    "        'routing': {\n",
    "            'agent': final_state.get('next_agent', 'unknown'),\n",
    "            'confidence': final_state.get('routing_confidence', 0.0),\n",
    "            'reasoning': final_state.get('routing_reasoning', '')\n",
    "        },\n",
    "        'clm_response': final_state.get('clm_response', ''),\n",
    "        'dc_response': final_state.get('dc_response', '')\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úÖ Query completed!\")\n",
    "    print(f\"   Routed to: {result['routing']['agent'].upper()}\")\n",
    "    print(f\"   Check trace at: https://smith.langchain.com/\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Query function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072430b-771f-4c84-bd8a-0547986a7653",
   "metadata": {},
   "source": [
    "## 10. Chat Interface (with LangSmith!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b58c1-a1d5-405a-b729-d3c793423a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import folium\n",
    "from folium import WmsTileLayer\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import base64\n",
    "import markdown\n",
    "import html as html_module\n",
    "\n",
    "class LangGraphChatInterface:\n",
    "    \"\"\"Chat interface for LangGraph multi-agent system.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages_container = []\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # Output area\n",
    "        self.output_area = widgets.VBox(\n",
    "            layout=widgets.Layout(\n",
    "                border='1px solid #ddd',\n",
    "                height='calc(100vh - 350px)',\n",
    "                min_height='400px',\n",
    "                overflow_y='auto',\n",
    "                padding='10px',\n",
    "                margin='0 0 10px 0'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Input controls\n",
    "        self.input_box = widgets.Textarea(\n",
    "            placeholder='Ask about California landscape metrics or general data...',\n",
    "            layout=widgets.Layout(width='100%', height='100px', margin='10px 0')\n",
    "        )\n",
    "        \n",
    "        self.send_button = widgets.Button(\n",
    "            description='Send',\n",
    "            button_style='primary',\n",
    "            icon='paper-plane',\n",
    "            layout=widgets.Layout(width='100px', margin='0 5px 0 0')\n",
    "        )\n",
    "        \n",
    "        self.clear_button = widgets.Button(\n",
    "            description='Clear',\n",
    "            button_style='warning',\n",
    "            icon='trash',\n",
    "            layout=widgets.Layout(width='100px', margin='0 5px 0 0')\n",
    "        )\n",
    "        \n",
    "        self.trace_button = widgets.Button(\n",
    "            description='View Trace',\n",
    "            button_style='info',\n",
    "            icon='chart-line',\n",
    "            layout=widgets.Layout(width='120px', margin='0 5px 0 0')\n",
    "        )\n",
    "        \n",
    "        self.status_label = widgets.HTML(\n",
    "            value=\"‚úÖ Ready\",\n",
    "            layout=widgets.Layout(margin='0 0 0 10px')\n",
    "        )\n",
    "        \n",
    "        # Event handlers\n",
    "        self.send_button.on_click(self.on_send_clicked)\n",
    "        self.clear_button.on_click(self.on_clear_clicked)\n",
    "        self.trace_button.on_click(self.on_trace_clicked)\n",
    "        \n",
    "        # Layout\n",
    "        button_box = widgets.HBox([\n",
    "            self.send_button,\n",
    "            self.clear_button,\n",
    "            self.trace_button,\n",
    "            self.status_label\n",
    "        ])\n",
    "        \n",
    "        self.interface = widgets.VBox([\n",
    "            widgets.HTML(value=\"\"\"\n",
    "                <h3>üéØ LangGraph Multi-Agent System</h3>\n",
    "                <p style='color: #666; font-size: 0.9em;'>\n",
    "                    <strong>‚ú® Powered by LangSmith Tracing!</strong><br>\n",
    "                    <strong>CLM Agent:</strong> California environmental data<br>\n",
    "                    <strong>Data Commons Agent:</strong> Global demographics<br>\n",
    "                    <strong>üîç View traces:</strong> <a href=\"https://smith.langchain.com/\" target=\"_blank\">smith.langchain.com</a>\n",
    "                </p>\n",
    "            \"\"\"),\n",
    "            self.output_area,\n",
    "            self.input_box,\n",
    "            button_box\n",
    "        ], layout=widgets.Layout(width='100%', max_width='1200px', margin='0 auto'))\n",
    "        \n",
    "        # Welcome message\n",
    "        self._add_message(\n",
    "            \"Welcome to LangGraph Multi-Agent System! üéâ\\n\\n\"\n",
    "            \"**New Features:**\\n\"\n",
    "            \"- üîç **LangSmith Tracing**: All interactions automatically logged\\n\"\n",
    "            \"- üìä **Visual Workflow**: See agent coordination in real-time\\n\"\n",
    "            \"- üí∞ **Cost Tracking**: Monitor token usage and costs\\n\"\n",
    "            \"- üêõ **Debug Mode**: Step through agent reasoning\\n\\n\"\n",
    "            \"**Try asking:**\\n\"\n",
    "            \"- What is the carbon turnover time in Los Angeles?\\n\"\n",
    "            \"- Show distribution of unemployment in San Diego\\n\"\n",
    "            \"- What is the population of Sacramento?\\n\\n\"\n",
    "            \"Click **View Trace** after each query to see the execution flow!\",\n",
    "            \"system\"\n",
    "        )\n",
    "    \n",
    "    def _create_distribution_chart(self, distribution_data):\n",
    "        \"\"\"Create distribution chart.\"\"\"\n",
    "        # Same implementation as before\n",
    "        try:\n",
    "            data = distribution_data.get('data', [])\n",
    "            dist_type = distribution_data.get('distribution_type', 'continuous')\n",
    "            dataset_info = distribution_data.get('dataset_info', {})\n",
    "            title = dataset_info.get('title', 'Value Distribution')\n",
    "            units = dataset_info.get('units', '')\n",
    "            filter_column = 'name'\n",
    "            \n",
    "            if not data:\n",
    "                return None\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, 6), dpi=100)\n",
    "            \n",
    "            counties = sorted(list(set([d.get(filter_column) for d in data if filter_column in d])))\n",
    "            if not counties:\n",
    "                return None\n",
    "            \n",
    "            colors = plt.cm.tab10(range(len(counties)))\n",
    "            \n",
    "            if dist_type == 'categorical':\n",
    "                import numpy as np\n",
    "                values = sorted(list(set([d['value'] for d in data])))\n",
    "                x = np.arange(len(values))\n",
    "                width = 0.8 / len(counties) if len(counties) > 1 else 0.5\n",
    "                \n",
    "                for i, county in enumerate(counties):\n",
    "                    county_data = [d for d in data if d.get(filter_column) == county]\n",
    "                    counts = []\n",
    "                    for val in values:\n",
    "                        matching = [d['count'] for d in county_data if d['value'] == val]\n",
    "                        counts.append(matching[0] if matching else 0)\n",
    "                    \n",
    "                    offset = (i - len(counties)/2) * width + width/2\n",
    "                    ax.bar(x + offset, counts, width, label=county, alpha=0.7, color=colors[i])\n",
    "                \n",
    "                ax.set_xlabel('Value', fontsize=11)\n",
    "                ax.set_ylabel('Count (pixels)', fontsize=11)\n",
    "                ax.set_xticks(x)\n",
    "                ax.set_xticklabels([str(v) for v in values])\n",
    "                ax.legend(fontsize=10, loc='best')\n",
    "                ax.set_title(f'{title}\\nCategorical Distribution', fontsize=12, fontweight='bold', pad=10)\n",
    "                \n",
    "            else:  # continuous\n",
    "                bins = distribution_data.get('bins', [])\n",
    "                if not bins:\n",
    "                    return None\n",
    "                \n",
    "                bin_centers = [(bins[i] + bins[i+1]) / 2 for i in range(len(bins)-1)]\n",
    "                bin_width = bins[1] - bins[0] if len(bins) > 1 else 1\n",
    "                bar_width = bin_width * 0.8 / len(counties) if len(counties) > 1 else bin_width * 0.7\n",
    "                \n",
    "                for i, county in enumerate(counties):\n",
    "                    county_data = [d for d in data if d.get(filter_column) == county]\n",
    "                    county_data = sorted(county_data, key=lambda x: x.get('bin_index', 0))\n",
    "                    counts = [d['count'] for d in county_data]\n",
    "                    \n",
    "                    if len(counties) > 1:\n",
    "                        offset = (i - len(counties)/2) * bar_width + bar_width/2\n",
    "                        positions = [bc + offset for bc in bin_centers]\n",
    "                    else:\n",
    "                        positions = bin_centers\n",
    "                    \n",
    "                    ax.bar(positions, counts, bar_width, label=county, alpha=0.7, color=colors[i])\n",
    "                \n",
    "                xlabel = f'Value Range ({units})' if units else 'Value Range'\n",
    "                ax.set_xlabel(xlabel, fontsize=11)\n",
    "                ax.set_ylabel('Count (pixels)', fontsize=11)\n",
    "                \n",
    "                if len(counties) > 1:\n",
    "                    ax.legend(fontsize=10, loc='best')\n",
    "                \n",
    "                ax.set_title(f'{title}\\nValue Distribution', fontsize=12, fontweight='bold', pad=10)\n",
    "            \n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            buf = io.BytesIO()\n",
    "            plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\n",
    "            buf.seek(0)\n",
    "            img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
    "            plt.close(fig)\n",
    "            \n",
    "            return img_base64\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating chart: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _create_map(self, map_data, style_name=None):\n",
    "        \"\"\"Create Folium map.\"\"\"\n",
    "        try:\n",
    "            wms_url = map_data.get('wms_base_url', '')\n",
    "            layer_name = map_data.get('wms_layer_name', '')\n",
    "            title = map_data.get('title', 'Dataset')\n",
    "            \n",
    "            # California bounds\n",
    "            bounds = [[32.5, -124.5], [42.0, -114.0]]\n",
    "            center_lat = (bounds[0][0] + bounds[1][0]) / 2\n",
    "            center_lon = (bounds[0][1] + bounds[1][1]) / 2\n",
    "            \n",
    "            m = folium.Map(\n",
    "                location=[center_lat, center_lon],\n",
    "                tiles='OpenStreetMap',\n",
    "                control_scale=True\n",
    "            )\n",
    "            m.fit_bounds(bounds)\n",
    "            \n",
    "            if wms_url and layer_name:\n",
    "                wms_params = {\n",
    "                    'url': wms_url + '/wms',\n",
    "                    'layers': layer_name,\n",
    "                    'name': title,\n",
    "                    'fmt': 'image/png',\n",
    "                    'transparent': True,\n",
    "                    'overlay': True,\n",
    "                    'control': True,\n",
    "                    'version': '1.1.0'\n",
    "                }\n",
    "                \n",
    "                if style_name:\n",
    "                    wms_params['styles'] = style_name\n",
    "                \n",
    "                wms = WmsTileLayer(**wms_params)\n",
    "                wms.add_to(m)\n",
    "                folium.LayerControl().add_to(m)\n",
    "            \n",
    "            return m\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating map: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _add_message(self, text, role=\"user\", map_data=None, distribution_data=None, routing_info=None):\n",
    "        \"\"\"Add message to chat.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        \n",
    "        if role == \"user\":\n",
    "            color = \"#007bff\"\n",
    "            icon = \"üë§\"\n",
    "            label = \"You\"\n",
    "            bg_color = \"#e7f3ff\"\n",
    "        elif role == \"assistant\":\n",
    "            color = \"#28a745\"\n",
    "            icon = \"ü§ñ\"\n",
    "            label = \"Assistant\"\n",
    "            bg_color = \"#e8f5e9\"\n",
    "        else:\n",
    "            color = \"#6c757d\"\n",
    "            icon = \"‚ÑπÔ∏è\"\n",
    "            label = \"System\"\n",
    "            bg_color = \"#f8f9fa\"\n",
    "        \n",
    "        # Add routing badge\n",
    "        routing_badge = \"\"\n",
    "        if routing_info:\n",
    "            agent = routing_info.get('agent', 'unknown').upper()\n",
    "            confidence = routing_info.get('confidence', 0)\n",
    "            agent_colors = {\n",
    "                'CLM': '#ff6b6b',\n",
    "                'DC': '#4ecdc4',\n",
    "                'BOTH': '#95e1d3'\n",
    "            }\n",
    "            badge_color = agent_colors.get(agent, '#999')\n",
    "            routing_badge = f\"\"\"\n",
    "                <span style='background-color: {badge_color}; color: white; padding: 2px 8px; \n",
    "                             border-radius: 12px; font-size: 0.75em; font-weight: bold; margin-left: 8px;'>\n",
    "                    {agent} ({confidence:.0%})\n",
    "                </span>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Convert markdown\n",
    "        if role == \"assistant\":\n",
    "            try:\n",
    "                html_content = markdown.markdown(str(text), extensions=['extra', 'nl2br', 'sane_lists'])\n",
    "            except:\n",
    "                html_content = html_module.escape(str(text)).replace('\\n', '<br>')\n",
    "        else:\n",
    "            html_content = html_module.escape(str(text)).replace('\\n', '<br>')\n",
    "        \n",
    "        message_html = widgets.HTML(\n",
    "            value=f\"\"\"\n",
    "            <div style='margin: 10px 0; padding: 12px; background-color: {bg_color}; \n",
    "                        border-radius: 8px; border-left: 4px solid {color}; box-shadow: 0 1px 3px rgba(0,0,0,0.1);'>\n",
    "                <div style='display: flex; justify-content: space-between; margin-bottom: 8px;'>\n",
    "                    <div>\n",
    "                        <strong style='color: {color};'>{icon} {label}</strong>\n",
    "                        {routing_badge}\n",
    "                    </div>\n",
    "                    <span style='color: #999; font-size: 0.85em;'>{timestamp}</span>\n",
    "                </div>\n",
    "                <div style='line-height: 1.6;'>{html_content}</div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        self.messages_container.append(message_html)\n",
    "        \n",
    "        # Add visualizations\n",
    "        if distribution_data:\n",
    "            img_base64 = self._create_distribution_chart(distribution_data)\n",
    "            if img_base64:\n",
    "                chart_html = f\"\"\"\n",
    "                <div style='width: 98%; margin: 10px 0;'>\n",
    "                    <div style='width: 100%; border: 1px solid #ddd; border-radius: 8px; \n",
    "                                padding: 10px; background-color: white;'>\n",
    "                        <img src=\"data:image/png;base64,{img_base64}\" \n",
    "                             style=\"width: 100%; height: auto;\" alt=\"Distribution Chart\">\n",
    "                    </div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "                self.messages_container.append(widgets.HTML(value=chart_html))\n",
    "        \n",
    "        elif map_data:\n",
    "            layer_name = map_data.get('wms_layer_name', '')\n",
    "            style_name = f\"{layer_name}_std\" if layer_name else None\n",
    "            folium_map = self._create_map(map_data, style_name=style_name)\n",
    "            \n",
    "            if folium_map:\n",
    "                map_html = f\"\"\"\n",
    "                <div style='width: 98%; margin: 10px 0;'>\n",
    "                    <div style='width: 100%; height: 300px; border: 1px solid #ddd; \n",
    "                                border-radius: 8px; overflow: hidden;'>\n",
    "                        {folium_map._repr_html_()}\n",
    "                    </div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "                self.messages_container.append(widgets.HTML(value=map_html))\n",
    "        \n",
    "        self.output_area.children = tuple(self.messages_container)\n",
    "    \n",
    "    def on_send_clicked(self, button):\n",
    "        \"\"\"Handle send button click.\"\"\"\n",
    "        question = self.input_box.value.strip()\n",
    "        if not question:\n",
    "            return\n",
    "        \n",
    "        self._add_message(question, \"user\")\n",
    "        self.input_box.value = \"\"\n",
    "        self.send_button.disabled = True\n",
    "        self.input_box.disabled = True\n",
    "        self.status_label.value = \"<span style='color: orange;'>‚è≥ Processing...</span>\"\n",
    "        \n",
    "        try:\n",
    "            import asyncio\n",
    "            \n",
    "            # Run the query\n",
    "            result = asyncio.get_event_loop().run_until_complete(\n",
    "                asyncio.wait_for(run_multi_agent_query(question), timeout=180)\n",
    "            )\n",
    "            \n",
    "            # Store in history\n",
    "            self.conversation_history.append({\n",
    "                'question': question,\n",
    "                'result': result,\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "            \n",
    "            answer = result.get('output', 'No response')\n",
    "            routing_info = result.get('routing')\n",
    "            map_data = result.get('map_data')\n",
    "            distribution_data = result.get('distribution_data')\n",
    "            \n",
    "            self._add_message(\n",
    "                answer,\n",
    "                \"assistant\",\n",
    "                map_data=map_data,\n",
    "                distribution_data=distribution_data,\n",
    "                routing_info=routing_info\n",
    "            )\n",
    "            \n",
    "            self.status_label.value = \"<span style='color: green;'>‚úÖ Ready</span>\"\n",
    "            \n",
    "        except asyncio.TimeoutError:\n",
    "            self._add_message(\"Request timed out after 3 minutes.\", \"system\")\n",
    "            self.status_label.value = \"<span style='color: red;'>‚ùå Timeout</span>\"\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_msg = f\"Error: {str(e)}\\n\\n{traceback.format_exc()}\"\n",
    "            self._add_message(error_msg, \"system\")\n",
    "            self.status_label.value = \"<span style='color: red;'>‚ùå Error</span>\"\n",
    "        finally:\n",
    "            self.send_button.disabled = False\n",
    "            self.input_box.disabled = False\n",
    "    \n",
    "    def on_clear_clicked(self, button):\n",
    "        \"\"\"Clear chat history.\"\"\"\n",
    "        self.messages_container = []\n",
    "        self.output_area.children = tuple(self.messages_container)\n",
    "        self._add_message(\n",
    "            \"Chat cleared. Start asking questions!\",\n",
    "            \"system\"\n",
    "        )\n",
    "    \n",
    "    def on_trace_clicked(self, button):\n",
    "        \"\"\"Show LangSmith trace link.\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            self._add_message(\n",
    "                \"No queries yet! Ask a question first, then click 'View Trace' to see the execution details.\",\n",
    "                \"system\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        last_query = self.conversation_history[-1]\n",
    "        trace_msg = f\"\"\"**üîç LangSmith Trace Available**\n",
    "\n",
    "View detailed trace at: [LangSmith Dashboard](https://smith.langchain.com/)\n",
    "\n",
    "**What you'll see:**\n",
    "- üìä Visual workflow graph\n",
    "- ‚è±Ô∏è Timing for each step\n",
    "- üí∞ Token usage and costs\n",
    "- üîß All tool calls and responses\n",
    "- üêõ Debug information\n",
    "\n",
    "**Last Query:**\n",
    "- Question: {last_query['question']}\n",
    "- Routed to: {last_query['result']['routing']['agent'].upper()}\n",
    "- Confidence: {last_query['result']['routing']['confidence']:.0%}\n",
    "\n",
    "All traces are automatically saved and searchable in LangSmith!\"\"\"\n",
    "        \n",
    "        self._add_message(trace_msg, \"system\")\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the interface.\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        display(HTML(\"\"\"\n",
    "        <style>\n",
    "            .jp-Cell-outputArea { max-height: none !important; }\n",
    "            .output_scroll { max-height: none !important; overflow-y: visible !important; }\n",
    "        </style>\n",
    "        \"\"\"))\n",
    "        display(self.interface)\n",
    "\n",
    "# Create and display chat interface\n",
    "chat = LangGraphChatInterface()\n",
    "chat.display()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Chat interface ready!\")\n",
    "print(\"All queries will be traced in LangSmith!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c949f8-3bc1-4767-885c-a1c6baf032bb",
   "metadata": {},
   "source": [
    "## 11. Example Queries and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed46ed0-4328-4992-963c-bd2fceb9d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Example queries to test the system\n",
    "example_queries = [\n",
    "    \"What is the carbon turnover time in Los Angeles?\",\n",
    "    \"Show me the unemployment distribution in San Diego\",\n",
    "    \"What is the population of Sacramento?\",\n",
    "    \"Compare burn probability between San Diego and Los Angeles\",\n",
    "    \"What is the median household income in San Francisco?\",\n",
    "]\n",
    "\n",
    "async def test_queries():\n",
    "    \"\"\"Test the system with example queries.\"\"\"\n",
    "    print(\"üß™ Testing Multi-Agent System\\n\")\n",
    "    \n",
    "    for i, question in enumerate(example_queries[:2], 1):  # Test first 2\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Test {i}/{len(example_queries[:2])}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        result = await run_multi_agent_query(question)\n",
    "        \n",
    "        print(f\"\\nResult:\")\n",
    "        print(f\"   Output: {result['output'][:200]}...\")\n",
    "        print(f\"   Agent: {result['routing']['agent'].upper()}\")\n",
    "        print(f\"   Confidence: {result['routing']['confidence']:.0%}\")\n",
    "        \n",
    "        if result.get('map_data'):\n",
    "            print(f\"   üìç Map data available\")\n",
    "        if result.get('distribution_data'):\n",
    "            print(f\"   üìä Distribution data available\")\n",
    "        \n",
    "        print(f\"\\nüîç View trace: https://smith.langchain.com/\")\n",
    "        \n",
    "        # Small delay between queries\n",
    "        await asyncio.sleep(2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Testing complete!\")\n",
    "    print(\"üîç Check LangSmith for detailed traces of all queries\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Uncomment to run tests:\n",
    "await test_queries()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d39da-f45e-4a4e-b84c-7ccf2a602c0a",
   "metadata": {},
   "source": [
    "## 12. Visualization Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f93fc-9f44-4bf5-86e3-7e1c6030571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client as LangSmithClient\n",
    "import os\n",
    "\n",
    "def get_trace_stats():\n",
    "    \"\"\"Get statistics from LangSmith traces.\"\"\"\n",
    "    try:\n",
    "        client = LangSmithClient()\n",
    "        project_name = os.environ.get(\"LANGCHAIN_PROJECT\", \"pr-shadowy-oleo-13\")\n",
    "\n",
    "        # Fetch the project to get tenant_id and project_id\n",
    "        project = client.read_project(project_name=project_name)\n",
    "        tenant_id = project.tenant_id\n",
    "        project_id = project.id\n",
    "\n",
    "        # Get recent runs\n",
    "        runs = list(client.list_runs(project_name=project_name, limit=10))\n",
    "\n",
    "        if not runs:\n",
    "            print(\"No traces found yet. Run some queries first!\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nüìä Trace Statistics for Project: {project_name}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        total_tokens = 0\n",
    "        total_cost = 0\n",
    "\n",
    "        for run in runs:\n",
    "            tokens = getattr(run, \"total_tokens\", None)\n",
    "            if tokens:\n",
    "                total_tokens += tokens\n",
    "                total_cost += (tokens / 1_000_000) * 0.375\n",
    "\n",
    "        print(f\"Recent Queries: {len(runs)}\")\n",
    "        print(f\"Total Tokens: {total_tokens:,}\")\n",
    "        print(f\"Estimated Cost: ${total_cost:.4f}\")\n",
    "\n",
    "        # Correct UI link\n",
    "        ui_url = (\n",
    "            f\"https://smith.langchain.com/o/{tenant_id}\"\n",
    "            f\"/projects/p/{project_id}\"\n",
    "        )\n",
    "        print(\"\\nüîó View all traces:\")\n",
    "        print(f\"   {ui_url}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting trace stats: {e}\")\n",
    "        print(\"Make sure LANGCHAIN_API_KEY / LANGSMITH_API_KEY is set correctly.\")\n",
    "\n",
    "# Usage\n",
    "get_trace_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d90e7-c1e8-4ee7-a6c0-2983f859a703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
